{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Model Selection (Qwen3 → GPT-OSS)\n",
    "\n",
    "This notebook demonstrates a cost-optimization strategy where the agent automatically switches between models based on conversation complexity.\n",
    "\n",
    "## Key Concepts\n",
    "- **Cost Optimization**: Use cheaper model when possible\n",
    "- **Performance Scaling**: Better model for complex tasks\n",
    "- **Automatic Decision-Making**: No manual switching required\n",
    "\n",
    "## Selection Logic\n",
    "- **< 10 messages**: Use Qwen3 (fast, efficient)\n",
    "- **≥ 10 messages**: Use GPT-OSS (better reasoning, longer context)\n",
    "\n",
    "## Real-World Applications\n",
    "- Customer service bots (simple queries → Qwen3, complex issues → GPT-OSS)\n",
    "- Research assistants (quick facts → Qwen3, analysis → GPT-OSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama pull qwen3 -< this is the simplest one\n",
    "# ollama pull gpt-oss -< this one is the higher end model | good one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.runtime import Runtime\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Function\n",
    "\n",
    "This function automatically chooses between Qwen3 and GPT-OSS based on conversation length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model selection function defined!\n",
      "Logic: < 10 messages = Qwen3, >= 10 messages = GPT-OSS\n"
     ]
    }
   ],
   "source": [
    "# Define tool list for both models\n",
    "tool_list = [tools.web_search, tools.analyze_data]\n",
    "\n",
    "def select_model(state: AgentState, runtime: Runtime) -> ChatOllama:\n",
    "    \"\"\"Choose between Qwen3 and GPT-OSS based on conversation length.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    message_count = len(messages)\n",
    "    \n",
    "    if message_count < 10:\n",
    "        print(f\"  Using Qwen3 for {message_count} messages (efficient)\")\n",
    "        return ChatOllama(model=\"qwen3\", temperature=0.1).bind_tools(tool_list)\n",
    "    else:\n",
    "        print(f\"  Switching to GPT-OSS for {message_count} messages (advanced)\")\n",
    "        return ChatOllama(model=\"gpt-oss\", temperature=0.0, num_predict=2000).bind_tools(tool_list)\n",
    "\n",
    "print(\"Model selection function defined!\")\n",
    "print(\"Logic: < 10 messages = Qwen3, >= 10 messages = GPT-OSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dynamic Agent\n",
    "\n",
    "Create an agent that uses our dynamic model selection function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic agent created successfully!\n",
      "This agent will automatically switch models based on conversation complexity\n"
     ]
    }
   ],
   "source": [
    "# Create agent with dynamic model selection\n",
    "agent = create_agent(select_model, tools=tool_list)\n",
    "\n",
    "print(\"Dynamic agent created successfully!\")\n",
    "print(\"This agent will automatically switch models based on conversation complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Short Conversation (Qwen3)\n",
    "\n",
    "Let's test with a simple query that should use Qwen3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Short Conversation (Should Use Qwen3) ===\n",
      "  Using Qwen3 for 1 messages (efficient)\n",
      "  Using Qwen3 for 3 messages (efficient)\n",
      "\n",
      "Short conversation result: <think>\n",
      "Okay, let me process these search results for AI news. The user asked for AI news, so I need to filter out any irrelevant links. The first result is about Air New Zealand, which doesn't seem related to AI. The other four links are from MIT News, Nature, Reuters, and CNBC, which are all credible sources.\n",
      "\n",
      "Looking at the second result, MIT News has an article about a new generative AI approach for predicting chemical reactions. That's definitely relevant. The third link from Nature discusses AI in materials discovery, which is a hot topic. The fourth one is about Bollywood stars and AI, which might be more about AI's impact on entertainment and privacy. The fifth link from CNBC talks about Microsoft's AI chips, which is tech news related to AI infrastructure.\n",
      "\n",
      "I should summarize each of these, making sure to highlight the key points. The user might be interested in various aspects of AI, so covering different areas like research, applications, and industry developments would be good. I need to present the information clearly, maybe bullet points for each article with a brief description. Also, include the URLs so the user can read more if interested. Let me check if there's any duplicate content or if some links are more important than others. All seem unique and relevant. Alright, time to put it all together in a concise manner.\n",
      "</think>\n",
      "\n",
      "Here are the top AI-related news summaries from the search results:\n",
      "\n",
      "1. **MIT News**  \n",
      "   A new generative AI system developed at MIT can predict chemical reactions with high accuracy, potentially accelerating drug discovery and materials science.  \n",
      "   [Read more](https://news.mit.edu/topic/artificial-intelligence2)\n",
      "\n",
      "2. **Nature Magazine**  \n",
      "   AI is being used to design millions of new materials, though critics argue the hype around projects by Google, Microsoft, and Meta may outpace practical progress.  \n",
      "   [Read more](https://www.nature.com/articles/d41586-025-03147-9)\n",
      "\n",
      "3. **Reuters**  \n",
      "   Bollywood celebrities in India are suing Google to protect their voice and persona rights from AI-driven deepfake technologies.  \n",
      "   [Read more](https://www.reuters.com/sustainability/society-equity/spooked-by-ai-bollywood-stars-drag-google-into-fight-personality-rights-2025-10-01/)\n",
      "\n",
      "4. **CNBC**  \n",
      "   Microsoft is shifting toward using its own AI chips in data centers to handle machine learning workloads, reducing reliance on third-party hardware.  \n",
      "   [Read more](https://www.cnbc.com/2025/10/01/microsoft-wants-to-mainly-use-its-own-ai-chips-in-the-future.html)\n",
      "\n",
      "Let me know if you'd like further details on any of these stories!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Testing Short Conversation (Should Use Qwen3) ===\")\n",
    "\n",
    "result1 = agent.invoke({\n",
    "    \"messages\": \"Search for AI news\"\n",
    "})\n",
    "\n",
    "print(f\"\\nShort conversation result: {result1['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Long Conversation (GPT-OSS)\n",
    "\n",
    "Now let's simulate a longer conversation that should trigger GPT-OSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Long Conversation (Should Use GPT-OSS) ===\n",
      "  Switching to GPT-OSS for 13 messages (advanced)\n",
      "\n",
      "Long conversation triggered model switch to GPT-OSS\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Testing Long Conversation (Should Use GPT-OSS) ===\")\n",
    "\n",
    "# Simulate conversation state with many messages\n",
    "long_messages = \"This is message number 12 in our conversation. I need complex analysis.\"\n",
    "\n",
    "# Create a new agent instance for this test\n",
    "agent_with_history = create_agent(select_model, tools=tool_list)\n",
    "\n",
    "result2 = agent_with_history.invoke({\n",
    "    \"messages\": [f\"Message {i}\" for i in range(12)] + [long_messages]\n",
    "})\n",
    "\n",
    "print(\"\\nLong conversation triggered model switch to GPT-OSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "\n",
    "Let's create an interactive demo where you can see the model switching in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Message 1: 'Hello' ===\n",
      "Would use Qwen3 (1 messages)\n",
      "  Using Qwen3 for 1 messages (efficient)\n",
      "\n",
      "=== Message 2: 'How are you?' ===\n",
      "Would use Qwen3 (2 messages)\n",
      "  Using Qwen3 for 2 messages (efficient)\n",
      "\n",
      "=== Message 3: 'What's the weather?' ===\n",
      "Would use Qwen3 (3 messages)\n",
      "  Using Qwen3 for 3 messages (efficient)\n",
      "\n",
      "=== Message 4: 'Tell me about AI' ===\n",
      "Would use Qwen3 (4 messages)\n",
      "  Using Qwen3 for 4 messages (efficient)\n",
      "\n",
      "=== Message 5: 'Explain machine learning' ===\n",
      "Would use Qwen3 (5 messages)\n",
      "  Using Qwen3 for 5 messages (efficient)\n",
      "  Using Qwen3 for 9 messages (efficient)\n",
      "\n",
      "=== Message 6: 'What about deep learning?' ===\n",
      "Would use Qwen3 (6 messages)\n",
      "  Using Qwen3 for 6 messages (efficient)\n",
      "\n",
      "=== Message 7: 'Show me examples' ===\n",
      "Would use Qwen3 (7 messages)\n",
      "  Using Qwen3 for 7 messages (efficient)\n",
      "  Using Qwen3 for 9 messages (efficient)\n",
      "\n",
      "=== Message 8: 'How does this work?' ===\n",
      "Would use Qwen3 (8 messages)\n",
      "  Using Qwen3 for 8 messages (efficient)\n",
      "  Switching to GPT-OSS for 10 messages (advanced)\n",
      "\n",
      "=== Message 9: 'Give me more details' ===\n",
      "Would use Qwen3 (9 messages)\n",
      "  Using Qwen3 for 9 messages (efficient)\n",
      "\n",
      "=== Message 10: 'I need comprehensive analysis' ===\n",
      "Would use GPT-OSS (10 messages) - SWITCHED!\n",
      "  Switching to GPT-OSS for 10 messages (advanced)\n"
     ]
    }
   ],
   "source": [
    "def demo_conversation_progression():\n",
    "    \"\"\"Demonstrate how the agent switches models as conversation grows.\"\"\"\n",
    "    conversation_messages = []\n",
    "    \n",
    "    # Simulate a growing conversation\n",
    "    test_messages = [\n",
    "        \"Hello\", \"How are you?\", \"What's the weather?\", \"Tell me about AI\",\n",
    "        \"Explain machine learning\", \"What about deep learning?\", \"Show me examples\",\n",
    "        \"How does this work?\", \"Give me more details\", \"I need comprehensive analysis\",\n",
    "        \"Please provide research data\", \"Analyze this thoroughly\"\n",
    "    ]\n",
    "    \n",
    "    for i, message in enumerate(test_messages, 1):\n",
    "        conversation_messages.append(message)\n",
    "        \n",
    "        print(f\"\\n=== Message {i}: '{message}' ===\")\n",
    "        \n",
    "        # Create a mock state to test model selection\n",
    "        mock_state = {\"messages\": conversation_messages}\n",
    "        \n",
    "        # Show which model would be selected\n",
    "        if len(conversation_messages) < 10:\n",
    "            print(f\"Would use Qwen3 ({len(conversation_messages)} messages)\")\n",
    "            agent = create_agent(select_model, tools=tool_list)\n",
    "            agent.invoke(mock_state)\n",
    "        else:\n",
    "            print(f\"Would use GPT-OSS ({len(conversation_messages)} messages) - SWITCHED!\")\n",
    "            agent = create_agent(select_model, tools=tool_list)\n",
    "            agent.invoke(mock_state)\n",
    "            break  # Stop demo after switch\n",
    "\n",
    "demo_conversation_progression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-v1-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

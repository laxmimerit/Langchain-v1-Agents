{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Methods of Prompt Configuration\n",
    "\n",
    "This notebook demonstrates different approaches to configuring agent prompts, from simple strings to dynamic, context-aware prompts.\n",
    "\n",
    "## Key Concepts\n",
    "- **Method 1**: String Prompt (simplest)\n",
    "- **Method 2**: SystemMessage (structured)\n",
    "- **Method 3**: Callable/Dynamic Prompt (advanced)\n",
    "\n",
    "## When to Use Each Method\n",
    "- **String**: Simple, static agents\n",
    "- **SystemMessage**: Production chat agents\n",
    "- **Callable**: Adaptive, personalized agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Simple String Prompt\n",
    "\n",
    "The simplest approach - direct string instruction to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Method 1: Simple String Prompt ===\")\n",
    "\n",
    "# Create model instance\n",
    "model = ChatOllama(model=\"qwen3\")\n",
    "\n",
    "# Method 1: String prompt (simplest)\n",
    "agent1 = create_agent(\n",
    "    model,\n",
    "    [tools.helper_tool],\n",
    "    prompt=\"You are a helpful assistant. Be concise and accurate in your responses.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent 1 created with string prompt\")\n",
    "print(\"  Characteristics:\")\n",
    "print(\"  - Direct string instruction\")\n",
    "print(\"  - Simple and readable\")\n",
    "print(\"  - Good for basic use cases\")\n",
    "print(\"  - Easy to modify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: SystemMessage Prompt\n",
    "\n",
    "Using LangChain's SystemMessage class for more structured prompt handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Method 2: SystemMessage Prompt ===\")\n",
    "\n",
    "# Method 2: SystemMessage prompt (structured)\n",
    "agent2 = create_agent(\n",
    "    model,\n",
    "    [tools.helper_tool],\n",
    "    prompt=SystemMessage(content=\"You are a research assistant. Always cite your sources and provide detailed explanations.\")\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent 2 created with SystemMessage prompt\")\n",
    "print(\"  Characteristics:\")\n",
    "print(\"  - Structured message object\")\n",
    "print(\"  - Better integration with chat models\")\n",
    "print(\"  - More explicit about message type\")\n",
    "print(\"  - Professional for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Dynamic Callable Prompt\n",
    "\n",
    "A function that generates prompts based on state - the most flexible approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Method 3: Dynamic Callable Prompt ===\")\n",
    "\n",
    "# Method 3: Callable/Dynamic prompt (most flexible)\n",
    "def dynamic_prompt(state):\n",
    "    \"\"\"Generate prompt based on user type and context.\"\"\"\n",
    "    user_type = state.get(\"user_type\", \"standard\")\n",
    "    \n",
    "    if user_type == \"expert\":\n",
    "        system_msg = SystemMessage(content=\"Provide detailed technical responses with code examples and advanced concepts.\")\n",
    "    elif user_type == \"beginner\":\n",
    "        system_msg = SystemMessage(content=\"Provide simple, clear explanations suitable for beginners. Use analogies and avoid jargon.\")\n",
    "    else:  # standard\n",
    "        system_msg = SystemMessage(content=\"Provide balanced explanations that are informative but accessible.\")\n",
    "    \n",
    "    return [system_msg] + state[\"messages\"]\n",
    "\n",
    "agent3 = create_agent(model, [tools.helper_tool], prompt=dynamic_prompt)\n",
    "\n",
    "print(\"âœ“ Agent 3 created with dynamic callable prompt\")\n",
    "print(\"  Characteristics:\")\n",
    "print(\"  - Adapts to user context\")\n",
    "print(\"  - Personalized responses\")\n",
    "print(\"  - Most flexible approach\")\n",
    "print(\"  - Perfect for adaptive systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing All Three Agents\n",
    "\n",
    "Let's test all agents with the same question to see their different behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question for all agents\n",
    "test_question = \"Help me understand artificial intelligence\"\n",
    "\n",
    "print(f\"Testing all agents with: '{test_question}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test Agent 1 (String prompt)\n",
    "print(\"\\n=== Agent 1 Response (String Prompt) ===\")\n",
    "print(\"Expected: Concise and accurate response\")\n",
    "\n",
    "result1 = agent1.invoke({\"messages\": test_question})\n",
    "print(f\"Response: {result1['messages'][-1].content}\")\n",
    "\n",
    "# Test Agent 2 (SystemMessage prompt)\n",
    "print(\"\\n=== Agent 2 Response (SystemMessage Prompt) ===\")\n",
    "print(\"Expected: Detailed explanation with research focus\")\n",
    "\n",
    "result2 = agent2.invoke({\"messages\": test_question})\n",
    "print(f\"Response: {result2['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dynamic Prompt with Different User Types\n",
    "\n",
    "The dynamic prompt agent can adapt based on user context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Agent 3 Responses (Dynamic Prompt) ===\")\n",
    "\n",
    "# Test with expert mode\n",
    "print(\"\\n--- Expert Mode ---\")\n",
    "print(\"Expected: Technical response with advanced concepts\")\n",
    "\n",
    "result3_expert = agent3.invoke({\"messages\": test_question,\"user_type\": \"expert\"})\n",
    "print(f\"Response: {result3_expert['messages'][-1].content}\")\n",
    "\n",
    "# Test with beginner mode\n",
    "print(\"\\n--- Beginner Mode ---\")\n",
    "print(\"Expected: Simple explanation with analogies\")\n",
    "\n",
    "result3_beginner = agent3.invoke({\"messages\": test_question,\"user_type\": \"beginner\"})\n",
    "print(f\"Response: {result3_beginner['messages'][-1].content}\")\n",
    "\n",
    "# Test with standard mode (default)\n",
    "print(\"\\n--- Standard Mode ---\")\n",
    "print(\"Expected: Balanced explanation\")\n",
    "\n",
    "result3_standard = agent3.invoke({\"messages\": test_question,\"user_type\": \"standard\"})\n",
    "print(f\"Response: {result3_standard['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice how each agent responds differently based on its prompt configuration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-v1-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short-Term Memory Management\n",
    "\n",
    "This notebook demonstrates various short-term memory management techniques for LangChain agents.\n",
    "\n",
    "## Key Concepts\n",
    "- **InMemorySaver**: Store conversation history in memory\n",
    "- **Thread Management**: Separate conversations for different users\n",
    "- **Message Trimming**: Keep only recent messages to manage context length\n",
    "- **Message Deletion**: Remove old messages from history\n",
    "- **Message Summarization**: Condense conversation history while preserving context\n",
    "\n",
    "## Use Cases\n",
    "- Multi-user chat applications\n",
    "- Long conversations that exceed token limits\n",
    "- Memory optimization for production systems\n",
    "- Context-aware chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.messages import RemoveMessage, BaseMessage\n",
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: InMemoryStore for Multiple Users\n",
    "\n",
    "This example demonstrates how to maintain separate conversation histories for different users using thread IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent created with InMemorySaver\n",
      "  - Maintains separate conversation histories per thread_id\n",
      "  - Perfect for multi-user applications\n"
     ]
    }
   ],
   "source": [
    "# Example 1: InMemoryStore for two different users\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    ChatOllama(model=\"qwen3\"),\n",
    "    tools=[],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent created with InMemorySaver\")\n",
    "print(\"  - Maintains separate conversation histories per thread_id\")\n",
    "print(\"  - Perfect for multi-user applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with User 1: kgptalkie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kgptalkie: <think>\n",
      "Okay, the user asked, \"What's my name?\" Let me think. Earlier, they introduced themselves as kgptalkie. So their name is kgptalkie. I should confirm that. But wait, maybe they're testing if I remember. I need to make sure I'm not making a mistake. Let me check the conversation history again. Yes, the first message was \"Hi! My name is kgptalkie.\" So their name is definitely kgptalkie. I should respond by stating their name and maybe offer further help. Keep it friendly and open-ended.\n",
      "</think>\n",
      "\n",
      "Your name is kgptalkie! ðŸ˜Š How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# User 1: kgptalkie\n",
    "agent.invoke(\n",
    "    {\"messages\": \"Hi! My name is kgptalkie.\"},\n",
    "    {\"configurable\": {\"thread_id\": \"kgptalkie\"}}\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": \"What's my name?\"},\n",
    "    {\"configurable\": {\"thread_id\": \"kgptalkie\"}}\n",
    ")\n",
    "print(\"kgptalkie:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with User 2: laxmikant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laxmikant: <think>\n",
      "Okay, the user asked, \"What's my name?\" after introducing themselves as Laxmikant. Let me check the conversation history. The user first said, \"Hi! My name is laxmikant.\" Then the assistant responded with a greeting and asked how they were doing. Now the user is asking for their name again.\n",
      "\n",
      "Wait, maybe the user is testing if I remember their name from the previous message. The assistant should have already known their name from the initial introduction. So the correct response is to confirm their name as Laxmikant. But I need to make sure there's no confusion. The user might be checking if the assistant retained the information. The assistant should acknowledge their name and maybe offer further help. Let me structure the response to be friendly and confirm the name while inviting them to ask more questions.\n",
      "</think>\n",
      "\n",
      "Your name is **Laxmikant**! ðŸ˜Š How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# User 2: laxmikant\n",
    "agent.invoke(\n",
    "    {\"messages\": \"Hi! My name is laxmikant.\"},\n",
    "    {\"configurable\": {\"thread_id\": \"laxmikant\"}}\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": \"What's my name?\"},\n",
    "    {\"configurable\": {\"thread_id\": \"laxmikant\"}}\n",
    ")\n",
    "print(\"laxmikant:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Trim Messages\n",
    "\n",
    "When conversations get too long, we can trim older messages to stay within token limits while keeping recent context.\n",
    "\n",
    "### Key Parameters\n",
    "- **strategy**: \"last\" keeps the most recent messages\n",
    "- **max_tokens**: Maximum tokens to keep (384 in this example)\n",
    "- **start_on**: Start trimming from messages of this type\n",
    "- **end_on**: End trimming on messages of these types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent created with message trimming\n",
      "  - Automatically trims messages to stay within token limits\n",
      "  - Keeps most recent context\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Trim messages\n",
    "def pre_model_hook(state) -> dict[str, list[BaseMessage]]:\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    ChatOllama(model=\"qwen3\"),\n",
    "    tools=[],\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent created with message trimming\")\n",
    "print(\"  - Automatically trims messages to stay within token limits\")\n",
    "print(\"  - Keeps most recent context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trim result: <think>\n",
      "Okay, the user asked, \"what's my name?\" I need to figure out how to respond. First, I should check if there's any context or previous conversation where the user mentioned their name. Since this is the first interaction, there's no prior history. In that case, I can't know their name unless they tell me. I should politely ask them to provide their name so I can address them properly. It's important to be friendly and helpful. Maybe I can phrase it as a question to encourage them to share their name. Let me make sure the response is clear and not too robotic. Also, I should avoid making assumptions. Yeah, that's the way to go.\n",
      "</think>\n",
      "\n",
      "I don't have access to your name unless you provide it. Could you please share your name so I can address you properly? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": \"hi, my name is bob\"},\n",
    "    {\"configurable\": {\"thread_id\": \"trim_example\"}}\n",
    ")\n",
    "agent.invoke(\n",
    "    {\"messages\": \"write a short poem about cats\"},\n",
    "    {\"configurable\": {\"thread_id\": \"trim_example\"}}\n",
    ")\n",
    "agent.invoke(\n",
    "    {\"messages\": \"now do the same but for dogs\"},\n",
    "    {\"configurable\": {\"thread_id\": \"trim_example\"}}\n",
    ")\n",
    "result = agent.invoke(\n",
    "    {\"messages\": \"what's my name?\"},\n",
    "    {\"configurable\": {\"thread_id\": \"trim_example\"}}\n",
    ")\n",
    "\n",
    "print(\"Trim result:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Delete Messages\n",
    "\n",
    "Instead of trimming, we can explicitly delete old messages after each response using a post-processing hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent created with message deletion\n",
      "  - Automatically deletes old messages after each response\n",
      "  - Keeps only the most recent messages\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Delete messages\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    ChatOllama(model=\"qwen3\"),\n",
    "    tools=[],\n",
    "    post_model_hook=delete_messages,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent created with message deletion\")\n",
    "print(\"  - Automatically deletes old messages after each response\")\n",
    "print(\"  - Keeps only the most recent messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete result: <think>\n",
      "Okay, the user just asked, \"what's my name?\" Let me think about how to respond. Earlier, the user introduced themselves as Alice. So, the name in question is Alice.\n",
      "\n",
      "I need to make sure I'm not confusing the user. The user might be testing if I remember their name from the previous conversation. Since I did acknowledge Alice as the user's name, I should confirm that. \n",
      "\n",
      "But wait, sometimes users might have multiple names or might be checking if I'm paying attention. However, in the conversation history, the user clearly stated their name as Alice. So the answer should be straightforward.\n",
      "\n",
      "I should respond in a friendly manner, reinforcing that I remember their name. Maybe add an emoji to keep the tone light. Also, offer further assistance in case they have more questions. Let me make sure the response is clear and positive.\n",
      "</think>\n",
      "\n",
      "Your name is Alice! ðŸ˜Š I remember that from our conversation earlier. How can I assist you today?\n",
      "Message count: 2\n"
     ]
    }
   ],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": \"hi! I'm alice\"},\n",
    "    {\"configurable\": {\"thread_id\": \"delete_example\"}}\n",
    ")\n",
    "result = agent.invoke(\n",
    "    {\"messages\": \"what's my name?\"},\n",
    "    {\"configurable\": {\"thread_id\": \"delete_example\"}}\n",
    ")\n",
    "\n",
    "print(\"Delete result:\", result[\"messages\"][-1].content)\n",
    "print(\"Message count:\", len(result[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete result: <think>\n",
      "Okay, the user is asking \"what's my name?\" again. Wait, in the previous message, I told them their name is Alice. But maybe they're testing if I remember or if there's a misunderstanding. Let me check the history.\n",
      "\n",
      "In the first message, the user asked the same question, and I responded that their name is Alice. Now they're asking again. Could they be confused, or is there a mistake? Maybe they don't remember or want to confirm. I should reaffirm their name again, but perhaps also ask if there's something specific they need help with. It's possible they're trying to see if I can recall their name correctly. I should make sure to be clear and friendly, offering further assistance. Let me respond by confirming their name and asking how I can help.\n",
      "</think>\n",
      "\n",
      "Your name is Alice! ðŸ˜Š I remember that from our conversation earlier. How can I assist you today?\n",
      "Message count: 2\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": \"what's my name?\"},\n",
    "    {\"configurable\": {\"thread_id\": \"delete_example\"}}\n",
    ")\n",
    "\n",
    "print(\"Delete result:\", result[\"messages\"][-1].content)\n",
    "print(\"Message count:\", len(result[\"messages\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Summarize Messages\n",
    "\n",
    "The most sophisticated approach: instead of deleting old messages, we summarize them to preserve important context while reducing token count.\n",
    "\n",
    "### Benefits\n",
    "- Preserves important conversation context\n",
    "- Reduces token usage\n",
    "- Maintains continuity in long conversations\n",
    "\n",
    "### Key Parameters\n",
    "- **max_tokens**: Maximum tokens before summarization (384)\n",
    "- **max_summary_tokens**: Maximum size of summary (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent created with message summarization\n",
      "  - Automatically summarizes old messages\n",
      "  - Preserves important context\n",
      "  - Reduces token usage\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Summarize messages\n",
    "from langmem.short_term import SummarizationNode, RunningSummary\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    context: dict[str, RunningSummary]\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\")\n",
    "\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=model,\n",
    "    max_tokens=384,\n",
    "    max_summary_tokens=128,\n",
    "    output_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    pre_model_hook=summarization_node,\n",
    "    state_schema=State,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "print(\"âœ“ Agent created with message summarization\")\n",
    "print(\"  - Automatically summarizes old messages\")\n",
    "print(\"  - Preserves important context\")\n",
    "print(\"  - Reduces token usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\YouTube\\Langchain-v1-Agents\\.venv\\Lib\\site-packages\\langmem\\short_term\\summarization.py:246: RuntimeWarning: Failed to trim messages to fit within max_tokens limit before summarization - falling back to the original message list. This may lead to exceeding the context window of the summarization LLM.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary result: <think>\n",
      "Okay, the user is asking, \"what's my name?\" Let me check the conversation history to see if there's any mention of their name. \n",
      "\n",
      "Looking back, the user initially introduced themselves as Charlie. Then they asked for a poem about cats, and the assistant provided one. Later, they asked for a similar poem about dogs, and the assistant did that too. Now, they're asking for their name. \n",
      "\n",
      "Wait, in the first message, the user said, \"Okay, the user asked for a summary of the conversation above. Let me check the history.\" Then they started the conversation with Charlie. So, the user's name is Charlie. But maybe they're testing if the assistant remembers. \n",
      "\n",
      "The assistant should confirm that the user's name is Charlie based on the initial introduction. However, the user might be checking if the assistant is paying attention. The response should be straightforward, stating that the user's name is Charlie, as that's what they introduced themselves as. No need to overcomplicate it. Just a simple confirmation.\n",
      "</think>\n",
      "\n",
      "Your name is Charlie. ðŸ˜Š\n",
      "\n",
      "\n",
      "Summary: <think>\n",
      "Okay, the user asked for a summary of the conversation above. Let me check the history. \n",
      "\n",
      "First, Charlie greeted the assistant, and the assistant responded warmly. Then Charlie asked for a poem about cats, and the assistant wrote a short poem titled \"Whiskers of the Moon.\" Now, the user wants a summary of that conversation.\n",
      "\n",
      "I need to make sure the summary includes the key points: Charlie introduced themselves, asked for a cat poem, and the assistant provided it. Also, mention the poem's title and its themes like mystery, grace, and the cats' characteristics. Keep it concise but cover all important elements. Let me structure it clearly without any markdown.\n",
      "</think>\n",
      "\n",
      "The conversation began with Charlie introducing themselves, after which the assistant greeted them warmly. Charlie then requested a short poem about cats, and the assistant responded with a whimsical, rhyming poem titled **\"Whiskers of the Moon\"**, capturing the mysterious, graceful, and playful nature of cats. The poem highlights their curious personalities, silent movements, and the enigmatic charm they bring to twilight moments.\n"
     ]
    }
   ],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": \"hi, my name is charlie\"},\n",
    "    {\"configurable\": {\"thread_id\": \"summary_example\"}}\n",
    ")\n",
    "agent.invoke(\n",
    "    {\"messages\": \"write a short poem about cats\"},\n",
    "    {\"configurable\": {\"thread_id\": \"summary_example\"}}\n",
    ")\n",
    "agent.invoke(\n",
    "    {\"messages\": \"now do the same but for dogs\"},\n",
    "    {\"configurable\": {\"thread_id\": \"summary_example\"}}\n",
    ")\n",
    "result = agent.invoke(\n",
    "    {\"messages\": \"what's my name?\"},\n",
    "    {\"configurable\": {\"thread_id\": \"summary_example\"}}\n",
    ")\n",
    "\n",
    "print(\"Summary result:\", result[\"messages\"][-1].content)\n",
    "print(\"\\n\\nSummary:\", result[\"context\"][\"running_summary\"].summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-v1-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

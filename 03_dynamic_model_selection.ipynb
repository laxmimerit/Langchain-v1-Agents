{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Model Selection (Qwen3 → GPT-OSS)\n",
    "\n",
    "This notebook demonstrates a cost-optimization strategy where the agent automatically switches between models based on conversation complexity.\n",
    "\n",
    "## Key Concepts\n",
    "- **Cost Optimization**: Use cheaper model when possible\n",
    "- **Performance Scaling**: Better model for complex tasks\n",
    "- **Automatic Decision-Making**: No manual switching required\n",
    "\n",
    "## Selection Logic\n",
    "- **< 10 messages**: Use Qwen3 (fast, efficient)\n",
    "- **≥ 10 messages**: Use GPT-OSS (better reasoning, longer context)\n",
    "\n",
    "## Real-World Applications\n",
    "- Customer service bots (simple queries → Qwen3, complex issues → GPT-OSS)\n",
    "- Research assistants (quick facts → Qwen3, analysis → GPT-OSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Make sure you have the required packages installed and both models available:\n",
    "\n",
    "```bash\n",
    "pip install --pre langchain langchain-community langchain-core langgraph pydantic\n",
    "ollama pull qwen3\n",
    "ollama pull gpt-oss\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.runtime import Runtime\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Function\n",
    "\n",
    "This function automatically chooses between Qwen3 and GPT-OSS based on conversation length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model selection function defined!\n",
      "Logic: < 10 messages = Qwen3, >= 10 messages = GPT-OSS\n"
     ]
    }
   ],
   "source": [
    "# Define tool list for both models\n",
    "tool_list = [tools.web_search, tools.analyze_data]\n",
    "\n",
    "def select_model(state: AgentState, runtime: Runtime) -> ChatOllama:\n",
    "    \"\"\"Choose between Qwen3 and GPT-OSS based on conversation length.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    message_count = len(messages)\n",
    "    \n",
    "    if message_count < 10:\n",
    "        print(f\"  Using Qwen3 for {message_count} messages (efficient)\")\n",
    "        return ChatOllama(model=\"qwen3\", temperature=0.1).bind_tools(tool_list)\n",
    "    else:\n",
    "        print(f\"  Switching to GPT-OSS for {message_count} messages (advanced)\")\n",
    "        return ChatOllama(model=\"gpt-oss\", temperature=0.0, num_predict=2000).bind_tools(tool_list)\n",
    "\n",
    "print(\"Model selection function defined!\")\n",
    "print(\"Logic: < 10 messages = Qwen3, >= 10 messages = GPT-OSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dynamic Agent\n",
    "\n",
    "Create an agent that uses our dynamic model selection function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic agent created successfully!\n",
      "This agent will automatically switch models based on conversation complexity\n"
     ]
    }
   ],
   "source": [
    "# Create agent with dynamic model selection\n",
    "agent = create_agent(select_model, tools=tool_list)\n",
    "\n",
    "print(\"Dynamic agent created successfully!\")\n",
    "print(\"This agent will automatically switch models based on conversation complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Short Conversation (Qwen3)\n",
    "\n",
    "Let's test with a simple query that should use Qwen3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Short Conversation (Should Use Qwen3) ===\n",
      "  Using Qwen3 for 1 messages (efficient)\n",
      "  Using Qwen3 for 3 messages (efficient)\n",
      "\n",
      "Short conversation result: <think>\n",
      "Okay, let me process these search results. The user asked for AI news, so I need to filter out any irrelevant links. The first result is about Air New Zealand, which doesn't seem related to AI news. The second link is from moneycontrol.com about an AI newsletter, which might be relevant. The third and fourth links discuss U.S. AI policies and AI's impact on jobs, which are definitely relevant. The fifth link is a finance article about an AI stock, which could be of interest. I should present the relevant ones, summarizing each with a title, description, and URL. Make sure to mention the sources and exclude the unrelated Air New Zealand article.\n",
      "</think>\n",
      "\n",
      "Here are the top AI-related news articles from the search results:\n",
      "\n",
      "1. **U.S. Rejects International AI Oversight at UN**  \n",
      "   The United States has opposed global AI regulation efforts at the UN General Assembly, sparking debates about AI governance.  \n",
      "   [Read more](https://www.nbcnews.com/tech/tech-news/us-rejects-international-ai-oversight-un-general-assembly-rcna233478)\n",
      "\n",
      "2. **AI Growth to Spark Serious Jobs Issues for Fed**  \n",
      "   Experts warn that the rapid advancement of AI could disrupt labor markets, prompting concerns about its impact on employment.  \n",
      "   [Read more](https://www.cnbc.com/2025/09/27/spectacular-ai-growth-to-spark-serious-jobs-issue-for-fed-zervos.html)\n",
      "\n",
      "3. **Prediction: AI Stock Will Dominate Markets**  \n",
      "   Analysts highlight Broadcom's AI division growth and its partnerships with tech giants to design custom AI chips.  \n",
      "   [Read more](https://finance.yahoo.com/news/prediction-artificial-intelligence-ai-stock-100000855.html)\n",
      "\n",
      "The first result about Air New Zealand was excluded as it is unrelated to AI news.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Testing Short Conversation (Should Use Qwen3) ===\")\n",
    "\n",
    "result1 = agent.invoke({\n",
    "    \"messages\": \"Search for AI news\"\n",
    "})\n",
    "\n",
    "print(f\"\\nShort conversation result: {result1['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Long Conversation (GPT-OSS)\n",
    "\n",
    "Now let's simulate a longer conversation that should trigger GPT-OSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Long Conversation (Should Use GPT-OSS) ===\n",
      "  Switching to GPT-OSS for 13 messages (advanced)\n",
      "\n",
      "Long conversation triggered model switch to GPT-OSS\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Testing Long Conversation (Should Use GPT-OSS) ===\")\n",
    "\n",
    "# Simulate conversation state with many messages\n",
    "long_messages = \"This is message number 12 in our conversation. I need complex analysis.\"\n",
    "\n",
    "# Create a new agent instance for this test\n",
    "agent_with_history = create_agent(select_model, tools=tool_list)\n",
    "\n",
    "result2 = agent_with_history.invoke({\n",
    "    \"messages\": [f\"Message {i}\" for i in range(12)] + [long_messages]\n",
    "})\n",
    "\n",
    "print(\"\\nLong conversation triggered model switch to GPT-OSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "\n",
    "Let's create an interactive demo where you can see the model switching in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Message 1: 'Hello' ===\n",
      "Would use Qwen3 (1 messages)\n",
      "\n",
      "=== Message 2: 'How are you?' ===\n",
      "Would use Qwen3 (2 messages)\n",
      "\n",
      "=== Message 3: 'What's the weather?' ===\n",
      "Would use Qwen3 (3 messages)\n",
      "\n",
      "=== Message 4: 'Tell me about AI' ===\n",
      "Would use Qwen3 (4 messages)\n",
      "\n",
      "=== Message 5: 'Explain machine learning' ===\n",
      "Would use Qwen3 (5 messages)\n",
      "\n",
      "=== Message 6: 'What about deep learning?' ===\n",
      "Would use Qwen3 (6 messages)\n",
      "\n",
      "=== Message 7: 'Show me examples' ===\n",
      "Would use Qwen3 (7 messages)\n",
      "\n",
      "=== Message 8: 'How does this work?' ===\n",
      "Would use Qwen3 (8 messages)\n",
      "\n",
      "=== Message 9: 'Give me more details' ===\n",
      "Would use Qwen3 (9 messages)\n",
      "\n",
      "=== Message 10: 'I need comprehensive analysis' ===\n",
      "Would use GPT-OSS (10 messages) - SWITCHED!\n"
     ]
    }
   ],
   "source": [
    "def demo_conversation_progression():\n",
    "    \"\"\"Demonstrate how the agent switches models as conversation grows.\"\"\"\n",
    "    conversation_messages = []\n",
    "    \n",
    "    # Simulate a growing conversation\n",
    "    test_messages = [\n",
    "        \"Hello\", \"How are you?\", \"What's the weather?\", \"Tell me about AI\",\n",
    "        \"Explain machine learning\", \"What about deep learning?\", \"Show me examples\",\n",
    "        \"How does this work?\", \"Give me more details\", \"I need comprehensive analysis\",\n",
    "        \"Please provide research data\", \"Analyze this thoroughly\"\n",
    "    ]\n",
    "    \n",
    "    for i, message in enumerate(test_messages, 1):\n",
    "        conversation_messages.append(message)\n",
    "        \n",
    "        print(f\"\\n=== Message {i}: '{message}' ===\")\n",
    "        \n",
    "        # Create a mock state to test model selection\n",
    "        mock_state = {\"messages\": conversation_messages}\n",
    "        \n",
    "        # Show which model would be selected\n",
    "        if len(conversation_messages) < 10:\n",
    "            print(f\"Would use Qwen3 ({len(conversation_messages)} messages)\")\n",
    "        else:\n",
    "            print(f\"Would use GPT-OSS ({len(conversation_messages)} messages) - SWITCHED!\")\n",
    "            break  # Stop demo after switch\n",
    "\n",
    "demo_conversation_progression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-v1-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

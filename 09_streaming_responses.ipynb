{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Agent Responses\n",
    "\n",
    "This notebook demonstrates how to stream agent responses in real-time, allowing you to see the agent's thinking process as it happens.\n",
    "\n",
    "## Key Concepts\n",
    "- **Real-time Feedback**: See responses as they develop\n",
    "- **Better UX**: Improved user experience for long-running tasks\n",
    "- **Intermediate Steps**: Observe reasoning and tool calls\n",
    "- **Early Termination**: Ability to stop if needed\n",
    "\n",
    "## Stream Modes\n",
    "- **\"values\"**: Get complete state at each step\n",
    "- **\"updates\"**: Get only the changes/updates\n",
    "- **\"messages\"**: Get only message updates\n",
    "\n",
    "## What You Can Stream\n",
    "- Model thinking/reasoning\n",
    "- Tool calls being made\n",
    "- Tool results/observations\n",
    "- Final answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Make sure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-core langgraph pydantic\n",
    "ollama pull qwen3\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import time\n",
    "import asyncio\n",
    "from typing import Dict, Any, AsyncGenerator\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Streaming Setup\n",
    "\n",
    "Let's start with a basic streaming agent configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Basic Streaming Setup ===\")\n",
    "\n",
    "# Create model and agent for streaming\n",
    "model = ChatOllama(model=\"qwen3\")\n",
    "streaming_agent = create_agent(\n",
    "    model, \n",
    "    tools=[tools.web_search, tools.calculate, tools.analyze_text]\n",
    ")\n",
    "\n",
    "print(\"‚úì Streaming agent created with multiple tools\")\n",
    "print(\"  Available tools: web_search, calculate, analyze_text\")\n",
    "print(\"  Ready for real-time streaming\")\n",
    "\n",
    "# Streaming utility functions\n",
    "class StreamingUtils:\n",
    "    \"\"\"Utilities for handling streaming responses.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_timestamp() -> str:\n",
    "        \"\"\"Get formatted timestamp for streaming output.\"\"\"\n",
    "        return time.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_content(message) -> str:\n",
    "        \"\"\"Safely extract content from message.\"\"\"\n",
    "        if hasattr(message, 'content') and message.content:\n",
    "            return str(message.content)\n",
    "        return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def identify_message_type(message) -> str:\n",
    "        \"\"\"Identify the type of streaming message.\"\"\"\n",
    "        if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "            return \"tool_call\"\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            return \"tool_result\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            return \"ai_response\"\n",
    "        elif isinstance(message, HumanMessage):\n",
    "            return \"human_input\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_tool_calls(tool_calls) -> str:\n",
    "        \"\"\"Format tool calls for display.\"\"\"\n",
    "        if not tool_calls:\n",
    "            return \"No tools called\"\n",
    "        \n",
    "        tool_names = []\n",
    "        for call in tool_calls:\n",
    "            if isinstance(call, dict):\n",
    "                tool_names.append(call.get('name', 'unknown_tool'))\n",
    "            else:\n",
    "                tool_names.append(getattr(call, 'name', 'unknown_tool'))\n",
    "        \n",
    "        return f\"[{', '.join(tool_names)}]\"\n",
    "\n",
    "print(\"\\n‚úì StreamingUtils helper class defined\")\n",
    "print(\"  - Timestamp formatting\")\n",
    "  print(\"  - Message content extraction\")\n",
    "print(\"  - Message type identification\")\n",
    "print(\"  - Tool call formatting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Streaming Example\n",
    "\n",
    "Let's demonstrate basic streaming with a multi-step query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Simple Streaming Example ===\")\n",
    "\n",
    "def simple_streaming_demo(query: str):\n",
    "    \"\"\"Demonstrate basic streaming functionality.\"\"\"\n",
    "    \n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Started at: {StreamingUtils.format_timestamp()}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    step_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Stream the agent's execution\n",
    "        for chunk in streaming_agent.stream({\n",
    "            \"messages\": query\n",
    "        }, stream_mode=\"values\"):\n",
    "            \n",
    "            step_count += 1\n",
    "            timestamp = StreamingUtils.format_timestamp()\n",
    "            \n",
    "            # Process the chunk\n",
    "            if \"messages\" in chunk and chunk[\"messages\"]:\n",
    "                latest_message = chunk[\"messages\"][-1]\n",
    "                message_type = StreamingUtils.identify_message_type(latest_message)\n",
    "                \n",
    "                print(f\"[{timestamp}] Step {step_count}: {message_type.replace('_', ' ').title()}\")\n",
    "                \n",
    "                # Handle different message types\n",
    "                if message_type == \"tool_call\":\n",
    "                    tools_called = StreamingUtils.format_tool_calls(latest_message.tool_calls)\n",
    "                    print(f\"  üîß Calling tools: {tools_called}\")\n",
    "                \n",
    "                elif message_type == \"tool_result\":\n",
    "                    print(f\"  üìä Tool completed: {getattr(latest_message, 'name', 'unknown')}\")\n",
    "                \n",
    "                elif message_type == \"ai_response\":\n",
    "                    content = StreamingUtils.extract_content(latest_message)\n",
    "                    if content:\n",
    "                        preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "                        print(f\"  üí≠ Agent thinking: {preview}\")\n",
    "            \n",
    "            # Small delay to make streaming visible\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"‚úì Streaming completed in {step_count} steps\")\n",
    "        print(f\"Finished at: {StreamingUtils.format_timestamp()}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Streaming error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Test with a multi-step query\n",
    "test_query = \"Search for the latest AI news, then calculate 25 * 4 + 100\"\n",
    "success = simple_streaming_demo(test_query)\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n‚ö†Ô∏è Streaming demo failed - falling back to regular invoke\")\n",
    "    try:\n",
    "        result = streaming_agent.invoke({\"messages\": test_query})\n",
    "        print(f\"‚úì Fallback response: {result['messages'][-1].content[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback also failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Streaming with State Tracking\n",
    "\n",
    "Let's create a more sophisticated streaming interface that tracks agent state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Streaming with State Tracking ===\")\n",
    "\n",
    "class AdvancedStreamingMonitor:\n",
    "    \"\"\"Advanced monitoring for streaming agent responses.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session_stats = {\n",
    "            \"start_time\": None,\n",
    "            \"total_steps\": 0,\n",
    "            \"tool_calls\": 0,\n",
    "            \"ai_responses\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"tools_used\": set(),\n",
    "            \"processing_times\": []\n",
    "        }\n",
    "        self.step_history = []\n",
    "        self.last_step_time = None\n",
    "    \n",
    "    def start_session(self):\n",
    "        \"\"\"Initialize a new streaming session.\"\"\"\n",
    "        self.session_stats[\"start_time\"] = time.time()\n",
    "        self.last_step_time = time.time()\n",
    "        print(f\"üöÄ Streaming session started at {StreamingUtils.format_timestamp()}\")\n",
    "    \n",
    "    def process_chunk(self, chunk: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a streaming chunk and update statistics.\"\"\"\n",
    "        current_time = time.time()\n",
    "        step_duration = current_time - self.last_step_time if self.last_step_time else 0\n",
    "        \n",
    "        self.session_stats[\"total_steps\"] += 1\n",
    "        self.session_stats[\"processing_times\"].append(step_duration)\n",
    "        \n",
    "        step_info = {\n",
    "            \"step_number\": self.session_stats[\"total_steps\"],\n",
    "            \"timestamp\": StreamingUtils.format_timestamp(),\n",
    "            \"duration_ms\": round(step_duration * 1000, 2),\n",
    "            \"chunk_type\": \"unknown\",\n",
    "            \"details\": {}\n",
    "        }\n",
    "        \n",
    "        # Analyze the chunk\n",
    "        if \"messages\" in chunk and chunk[\"messages\"]:\n",
    "            latest_message = chunk[\"messages\"][-1]\n",
    "            message_type = StreamingUtils.identify_message_type(latest_message)\n",
    "            step_info[\"chunk_type\"] = message_type\n",
    "            \n",
    "            # Update statistics based on message type\n",
    "            if message_type == \"tool_call\":\n",
    "                self.session_stats[\"tool_calls\"] += 1\n",
    "                if hasattr(latest_message, 'tool_calls'):\n",
    "                    for call in latest_message.tool_calls:\n",
    "                        tool_name = call.get('name', 'unknown') if isinstance(call, dict) else getattr(call, 'name', 'unknown')\n",
    "                        self.session_stats[\"tools_used\"].add(tool_name)\n",
    "                        step_info[\"details\"][\"tool_name\"] = tool_name\n",
    "            \n",
    "            elif message_type == \"ai_response\":\n",
    "                self.session_stats[\"ai_responses\"] += 1\n",
    "                content = StreamingUtils.extract_content(latest_message)\n",
    "                step_info[\"details\"][\"content_length\"] = len(content)\n",
    "                self.session_stats[\"total_tokens\"] += len(content.split()) if content else 0\n",
    "        \n",
    "        self.step_history.append(step_info)\n",
    "        self.last_step_time = current_time\n",
    "        \n",
    "        return step_info\n",
    "    \n",
    "    def display_step(self, step_info: Dict[str, Any]):\n",
    "        \"\"\"Display formatted step information.\"\"\"\n",
    "        step_num = step_info[\"step_number\"]\n",
    "        timestamp = step_info[\"timestamp\"]\n",
    "        duration = step_info[\"duration_ms\"]\n",
    "        chunk_type = step_info[\"chunk_type\"]\n",
    "        details = step_info[\"details\"]\n",
    "        \n",
    "        # Format the step display\n",
    "        type_emoji = {\n",
    "            \"tool_call\": \"üîß\",\n",
    "            \"tool_result\": \"üìä\", \n",
    "            \"ai_response\": \"üí≠\",\n",
    "            \"human_input\": \"üë§\",\n",
    "            \"unknown\": \"‚ùì\"\n",
    "        }\n",
    "        \n",
    "        emoji = type_emoji.get(chunk_type, \"‚ùì\")\n",
    "        type_name = chunk_type.replace('_', ' ').title()\n",
    "        \n",
    "        print(f\"[{timestamp}] Step {step_num}: {emoji} {type_name} ({duration}ms)\")\n",
    "        \n",
    "        # Show relevant details\n",
    "        if \"tool_name\" in details:\n",
    "            print(f\"  ‚îî‚îÄ Tool: {details['tool_name']}\")\n",
    "        elif \"content_length\" in details:\n",
    "            length = details['content_length']\n",
    "            print(f\"  ‚îî‚îÄ Response length: {length} characters\")\n",
    "    \n",
    "    def end_session(self) -> Dict[str, Any]:\n",
    "        \"\"\"End the session and return comprehensive statistics.\"\"\"\n",
    "        if not self.session_stats[\"start_time\"]:\n",
    "            return {\"error\": \"Session not started\"}\n",
    "        \n",
    "        total_duration = time.time() - self.session_stats[\"start_time\"]\n",
    "        \n",
    "        summary = {\n",
    "            \"session_duration_seconds\": round(total_duration, 2),\n",
    "            \"total_steps\": self.session_stats[\"total_steps\"],\n",
    "            \"tool_calls\": self.session_stats[\"tool_calls\"],\n",
    "            \"ai_responses\": self.session_stats[\"ai_responses\"],\n",
    "            \"estimated_tokens\": self.session_stats[\"total_tokens\"],\n",
    "            \"unique_tools_used\": len(self.session_stats[\"tools_used\"]),\n",
    "            \"tools_list\": list(self.session_stats[\"tools_used\"]),\n",
    "            \"average_step_time_ms\": round(sum(self.session_stats[\"processing_times\"]) * 1000 / len(self.session_stats[\"processing_times\"]), 2) if self.session_stats[\"processing_times\"] else 0,\n",
    "            \"steps_per_second\": round(self.session_stats[\"total_steps\"] / total_duration, 2) if total_duration > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "print(\"‚úì AdvancedStreamingMonitor created with comprehensive tracking:\")\n",
    "print(\"  - Session statistics and timing\")\n",
    "print(\"  - Step-by-step analysis\")\n",
    "print(\"  - Tool usage tracking\")\n",
    "print(\"  - Performance metrics\")\n",
    "print(\"  - Formatted display output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Advanced Streaming Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing Advanced Streaming Monitor ===\")\n",
    "\n",
    "def advanced_streaming_demo(query: str, monitor: AdvancedStreamingMonitor):\n",
    "    \"\"\"Demonstrate advanced streaming with comprehensive monitoring.\"\"\"\n",
    "    \n",
    "    monitor.start_session()\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Stream with advanced monitoring\n",
    "        for chunk in streaming_agent.stream({\n",
    "            \"messages\": query\n",
    "        }, stream_mode=\"values\"):\n",
    "            \n",
    "            # Process and display the chunk\n",
    "            step_info = monitor.process_chunk(chunk)\n",
    "            monitor.display_step(step_info)\n",
    "            \n",
    "            # Small delay for visibility\n",
    "            time.sleep(0.05)\n",
    "        \n",
    "        # Session summary\n",
    "        print(\"=\" * 70)\n",
    "        summary = monitor.end_session()\n",
    "        \n",
    "        print(\"üìà Session Summary:\")\n",
    "        print(f\"  Total Duration: {summary['session_duration_seconds']}s\")\n",
    "        print(f\"  Steps Processed: {summary['total_steps']}\")\n",
    "        print(f\"  Tool Calls: {summary['tool_calls']}\")\n",
    "        print(f\"  AI Responses: {summary['ai_responses']}\")\n",
    "        print(f\"  Estimated Tokens: {summary['estimated_tokens']}\")\n",
    "        print(f\"  Tools Used: {', '.join(summary['tools_list']) if summary['tools_list'] else 'None'}\")\n",
    "        print(f\"  Avg Step Time: {summary['average_step_time_ms']}ms\")\n",
    "        print(f\"  Processing Speed: {summary['steps_per_second']} steps/second\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Advanced streaming error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test with a complex multi-tool query\n",
    "complex_query = \"Search for Python machine learning tutorials, then analyze this text: 'Machine learning is transforming software development', and finally calculate the result of 15^2 + 25\"\n",
    "\n",
    "monitor = AdvancedStreamingMonitor()\n",
    "success = advanced_streaming_demo(complex_query, monitor)\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n‚ö†Ô∏è Advanced streaming demo failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Streaming for Better Performance\n",
    "\n",
    "Let's implement asynchronous streaming for improved performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Async Streaming Implementation ===\")\n",
    "\n",
    "class AsyncStreamingHandler:\n",
    "    \"\"\"Asynchronous streaming handler for better performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.active_streams = {}\n",
    "        self.stream_counter = 0\n",
    "    \n",
    "    async def stream_response(self, query: str, stream_id: str = None) -> AsyncGenerator[Dict[str, Any], None]:\n",
    "        \"\"\"Asynchronously stream agent responses.\"\"\"\n",
    "        \n",
    "        if not stream_id:\n",
    "            stream_id = f\"stream_{self.stream_counter}\"\n",
    "            self.stream_counter += 1\n",
    "        \n",
    "        self.active_streams[stream_id] = {\n",
    "            \"status\": \"active\",\n",
    "            \"start_time\": time.time(),\n",
    "            \"steps_processed\": 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Simulate async streaming (in real implementation, use actual async agent)\n",
    "            for chunk in self.agent.stream({\"messages\": query}, stream_mode=\"values\"):\n",
    "                \n",
    "                self.active_streams[stream_id][\"steps_processed\"] += 1\n",
    "                \n",
    "                # Yield chunk with metadata\n",
    "                yield {\n",
    "                    \"stream_id\": stream_id,\n",
    "                    \"step\": self.active_streams[stream_id][\"steps_processed\"],\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"data\": chunk\n",
    "                }\n",
    "                \n",
    "                # Allow other coroutines to run\n",
    "                await asyncio.sleep(0.01)\n",
    "            \n",
    "            # Mark stream as completed\n",
    "            self.active_streams[stream_id][\"status\"] = \"completed\"\n",
    "            self.active_streams[stream_id][\"end_time\"] = time.time()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.active_streams[stream_id][\"status\"] = \"error\"\n",
    "            self.active_streams[stream_id][\"error\"] = str(e)\n",
    "            \n",
    "            yield {\n",
    "                \"stream_id\": stream_id,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "    \n",
    "    async def process_multiple_queries(self, queries: list) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple queries concurrently.\"\"\"\n",
    "        \n",
    "        print(f\"üîÑ Processing {len(queries)} queries concurrently...\")\n",
    "        \n",
    "        # Create tasks for concurrent processing\n",
    "        tasks = []\n",
    "        for i, query in enumerate(queries):\n",
    "            stream_id = f\"concurrent_stream_{i}\"\n",
    "            task = asyncio.create_task(self._collect_stream_results(query, stream_id))\n",
    "            tasks.append((stream_id, task))\n",
    "        \n",
    "        # Wait for all tasks to complete\n",
    "        results = {}\n",
    "        for stream_id, task in tasks:\n",
    "            try:\n",
    "                results[stream_id] = await task\n",
    "            except Exception as e:\n",
    "                results[stream_id] = {\"error\": str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def _collect_stream_results(self, query: str, stream_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Collect all results from a stream.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        async for chunk in self.stream_response(query, stream_id):\n",
    "            results.append(chunk)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"total_chunks\": len(results),\n",
    "            \"duration\": time.time() - start_time,\n",
    "            \"final_status\": self.active_streams[stream_id][\"status\"]\n",
    "        }\n",
    "    \n",
    "    def get_stream_status(self, stream_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Get status of active streams.\"\"\"\n",
    "        if stream_id:\n",
    "            return self.active_streams.get(stream_id, {\"error\": \"Stream not found\"})\n",
    "        \n",
    "        return {\n",
    "            \"total_streams\": len(self.active_streams),\n",
    "            \"active_streams\": [sid for sid, info in self.active_streams.items() if info[\"status\"] == \"active\"],\n",
    "            \"completed_streams\": [sid for sid, info in self.active_streams.items() if info[\"status\"] == \"completed\"],\n",
    "            \"error_streams\": [sid for sid, info in self.active_streams.items() if info[\"status\"] == \"error\"]\n",
    "        }\n",
    "\n",
    "print(\"‚úì AsyncStreamingHandler created with capabilities:\")\n",
    "print(\"  - Asynchronous response streaming\")\n",
    "print(\"  - Concurrent query processing\")\n",
    "print(\"  - Stream status tracking\")\n",
    "print(\"  - Error handling and recovery\")\n",
    "print(\"  - Performance monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Async Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing Async Streaming ===\")\n",
    "\n",
    "async def test_async_streaming():\n",
    "    \"\"\"Test asynchronous streaming functionality.\"\"\"\n",
    "    \n",
    "    handler = AsyncStreamingHandler(streaming_agent)\n",
    "    \n",
    "    # Test 1: Single async stream\n",
    "    print(\"\\n--- Test 1: Single Async Stream ---\")\n",
    "    single_query = \"Calculate 10 * 5 and analyze the text 'Async streaming is powerful'\"\n",
    "    \n",
    "    print(f\"Query: {single_query}\")\n",
    "    print(\"Streaming results:\")\n",
    "    \n",
    "    chunk_count = 0\n",
    "    async for chunk in handler.stream_response(single_query):\n",
    "        chunk_count += 1\n",
    "        if \"error\" in chunk:\n",
    "            print(f\"  ‚ùå Error in chunk {chunk_count}: {chunk['error']}\")\n",
    "        else:\n",
    "            print(f\"  ‚úì Chunk {chunk_count}: Step {chunk.get('step', '?')} at {StreamingUtils.format_timestamp()}\")\n",
    "        \n",
    "        if chunk_count >= 10:  # Limit output for demo\n",
    "            break\n",
    "    \n",
    "    print(f\"Total chunks processed: {chunk_count}\")\n",
    "    \n",
    "    # Test 2: Multiple concurrent streams\n",
    "    print(\"\\n--- Test 2: Concurrent Streams ---\")\n",
    "    concurrent_queries = [\n",
    "        \"Calculate 2 + 2\",\n",
    "        \"Analyze text: 'Hello world'\",\n",
    "        \"Calculate 5 * 10\"\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = await handler.process_multiple_queries(concurrent_queries)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úì Concurrent processing completed in {total_time:.2f}s\")\n",
    "    \n",
    "    for stream_id, result in results.items():\n",
    "        if \"error\" in result:\n",
    "            print(f\"  ‚ùå {stream_id}: Error - {result['error']}\")\n",
    "        else:\n",
    "            print(f\"  ‚úì {stream_id}: {result['total_chunks']} chunks, {result['duration']:.2f}s, {result['final_status']}\")\n",
    "    \n",
    "    # Stream status summary\n",
    "    print(\"\\n--- Stream Status Summary ---\")\n",
    "    status = handler.get_stream_status()\n",
    "    print(f\"Total streams created: {status['total_streams']}\")\n",
    "    print(f\"Active streams: {len(status['active_streams'])}\")\n",
    "    print(f\"Completed streams: {len(status['completed_streams'])}\")\n",
    "    print(f\"Error streams: {len(status['error_streams'])}\")\n",
    "\n",
    "# Run the async test\n",
    "try:\n",
    "    await test_async_streaming()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Async streaming test failed: {e}\")\n",
    "    print(\"This might be expected in some environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with User Interaction\n",
    "\n",
    "Let's create an interactive streaming demo that allows user control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Interactive Streaming Demo ===\")\n",
    "\n",
    "class InteractiveStreamingDemo:\n",
    "    \"\"\"Interactive streaming demo with user controls.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.paused = False\n",
    "        self.step_by_step = False\n",
    "        self.max_steps = None\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def configure(self, step_by_step=False, max_steps=None):\n",
    "        \"\"\"Configure streaming behavior.\"\"\"\n",
    "        self.step_by_step = step_by_step\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        print(f\"\\nüîß Streaming configured:\")\n",
    "        print(f\"  - Step-by-step mode: {self.step_by_step}\")\n",
    "        print(f\"  - Max steps: {self.max_steps or 'unlimited'}\")\n",
    "    \n",
    "    def stream_with_controls(self, query: str):\n",
    "        \"\"\"Stream with user interaction controls.\"\"\"\n",
    "        \n",
    "        print(f\"\\nüé¨ Starting interactive stream for: '{query}'\")\n",
    "        \n",
    "        if self.step_by_step:\n",
    "            print(\"\\n‚èØÔ∏è  Step-by-step mode enabled\")\n",
    "            print(\"   Commands: 'c' = continue, 'p' = pause, 's' = stop, 'i' = info\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        self.current_step = 0\n",
    "        \n",
    "        try:\n",
    "            for chunk in self.agent.stream({\"messages\": query}, stream_mode=\"values\"):\n",
    "                \n",
    "                self.current_step += 1\n",
    "                \n",
    "                # Check step limit\n",
    "                if self.max_steps and self.current_step > self.max_steps:\n",
    "                    print(f\"\\nüõë Reached maximum steps ({self.max_steps}). Stopping stream.\")\n",
    "                    break\n",
    "                \n",
    "                # Process and display chunk\n",
    "                self._display_chunk(chunk)\n",
    "                \n",
    "                # Handle step-by-step mode\n",
    "                if self.step_by_step:\n",
    "                    action = self._get_user_action()\n",
    "                    if action == \"stop\":\n",
    "                        print(\"\\nüõë Stream stopped by user.\")\n",
    "                        break\n",
    "                    elif action == \"pause\":\n",
    "                        self._handle_pause()\n",
    "                \n",
    "                # Small delay for readability\n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "            print(f\"‚úÖ Stream completed. Total steps: {self.current_step}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚èπÔ∏è  Stream interrupted by user (Ctrl+C)\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Stream error: {e}\")\n",
    "    \n",
    "    def _display_chunk(self, chunk):\n",
    "        \"\"\"Display chunk information.\"\"\"\n",
    "        timestamp = StreamingUtils.format_timestamp()\n",
    "        \n",
    "        if \"messages\" in chunk and chunk[\"messages\"]:\n",
    "            latest_message = chunk[\"messages\"][-1]\n",
    "            message_type = StreamingUtils.identify_message_type(latest_message)\n",
    "            \n",
    "            type_icons = {\n",
    "                \"tool_call\": \"üîß\",\n",
    "                \"tool_result\": \"üìä\",\n",
    "                \"ai_response\": \"üí≠\",\n",
    "                \"human_input\": \"üë§\"\n",
    "            }\n",
    "            \n",
    "            icon = type_icons.get(message_type, \"‚ùì\")\n",
    "            type_name = message_type.replace('_', ' ').title()\n",
    "            \n",
    "            print(f\"[{timestamp}] Step {self.current_step}: {icon} {type_name}\")\n",
    "            \n",
    "            # Show additional details\n",
    "            if message_type == \"tool_call\" and hasattr(latest_message, 'tool_calls'):\n",
    "                tools = StreamingUtils.format_tool_calls(latest_message.tool_calls)\n",
    "                print(f\"  ‚îî‚îÄ Tools: {tools}\")\n",
    "            \n",
    "            elif message_type == \"ai_response\":\n",
    "                content = StreamingUtils.extract_content(latest_message)\n",
    "                if content:\n",
    "                    preview = content[:80] + \"...\" if len(content) > 80 else content\n",
    "                    print(f\"  ‚îî‚îÄ Response: {preview}\")\n",
    "    \n",
    "    def _get_user_action(self):\n",
    "        \"\"\"Get user action in step-by-step mode (simulated).\"\"\"\n",
    "        # In a real interactive environment, you would get actual user input\n",
    "        # For demo purposes, we'll just continue automatically\n",
    "        return \"continue\"\n",
    "    \n",
    "    def _handle_pause(self):\n",
    "        \"\"\"Handle pause functionality.\"\"\"\n",
    "        print(\"\\n‚è∏Ô∏è  Stream paused. Press any key to continue...\")\n",
    "        # In real implementation, wait for user input\n",
    "        time.sleep(1)  # Simulate pause\n",
    "        print(\"‚ñ∂Ô∏è  Resuming stream...\\n\")\n",
    "\n",
    "# Create and test interactive demo\n",
    "interactive_demo = InteractiveStreamingDemo(streaming_agent)\n",
    "\n",
    "# Test 1: Regular streaming\n",
    "print(\"\\n--- Test 1: Regular Interactive Streaming ---\")\n",
    "interactive_demo.configure(step_by_step=False, max_steps=None)\n",
    "interactive_demo.stream_with_controls(\"Calculate 7 * 8 and search for Python tutorials\")\n",
    "\n",
    "# Test 2: Step-by-step with limit\n",
    "print(\"\\n--- Test 2: Step-by-Step with Limits ---\")\n",
    "interactive_demo.configure(step_by_step=True, max_steps=5)\n",
    "interactive_demo.stream_with_controls(\"Analyze this text: 'Interactive streaming provides great user control'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis and Optimization\n",
    "\n",
    "Let's analyze streaming performance and provide optimization insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Performance Analysis and Optimization ===\")\n",
    "\n",
    "class StreamingPerformanceAnalyzer:\n",
    "    \"\"\"Analyze and optimize streaming performance.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.benchmarks = []\n",
    "        self.optimization_tips = {\n",
    "            \"high_latency\": \"Consider reducing model temperature or using a faster model\",\n",
    "            \"many_tool_calls\": \"Optimize tool implementations or batch operations\",\n",
    "            \"large_responses\": \"Implement response chunking or summarization\",\n",
    "            \"slow_tools\": \"Cache tool results or use async tool execution\",\n",
    "            \"memory_usage\": \"Implement state cleanup and garbage collection\"\n",
    "        }\n",
    "    \n",
    "    def benchmark_streaming(self, agent, queries: list, iterations: int = 3):\n",
    "        \"\"\"Benchmark streaming performance across multiple queries.\"\"\"\n",
    "        \n",
    "        print(f\"üèÉ Running streaming benchmark...\")\n",
    "        print(f\"  Queries: {len(queries)}\")\n",
    "        print(f\"  Iterations per query: {iterations}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for query_idx, query in enumerate(queries):\n",
    "            query_benchmarks = []\n",
    "            \n",
    "            print(f\"\\nQuery {query_idx + 1}: '{query[:50]}...'\")\n",
    "            \n",
    "            for iteration in range(iterations):\n",
    "                benchmark = self._benchmark_single_stream(agent, query, iteration + 1)\n",
    "                query_benchmarks.append(benchmark)\n",
    "                \n",
    "                print(f\"  Iteration {iteration + 1}: {benchmark['total_time']:.2f}s, {benchmark['total_steps']} steps\")\n",
    "            \n",
    "            # Calculate averages for this query\n",
    "            avg_benchmark = self._calculate_averages(query_benchmarks)\n",
    "            avg_benchmark['query'] = query\n",
    "            self.benchmarks.append(avg_benchmark)\n",
    "            \n",
    "            print(f\"  Average: {avg_benchmark['avg_total_time']:.2f}s, {avg_benchmark['avg_total_steps']:.1f} steps\")\n",
    "        \n",
    "        return self._generate_performance_report()\n",
    "    \n",
    "    def _benchmark_single_stream(self, agent, query: str, iteration: int):\n",
    "        \"\"\"Benchmark a single streaming session.\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        step_times = []\n",
    "        tool_calls = 0\n",
    "        ai_responses = 0\n",
    "        total_steps = 0\n",
    "        \n",
    "        last_step_time = start_time\n",
    "        \n",
    "        try:\n",
    "            for chunk in agent.stream({\"messages\": query}, stream_mode=\"values\"):\n",
    "                current_time = time.time()\n",
    "                step_duration = current_time - last_step_time\n",
    "                step_times.append(step_duration)\n",
    "                \n",
    "                total_steps += 1\n",
    "                \n",
    "                # Analyze chunk content\n",
    "                if \"messages\" in chunk and chunk[\"messages\"]:\n",
    "                    latest_message = chunk[\"messages\"][-1]\n",
    "                    message_type = StreamingUtils.identify_message_type(latest_message)\n",
    "                    \n",
    "                    if message_type == \"tool_call\":\n",
    "                        tool_calls += 1\n",
    "                    elif message_type == \"ai_response\":\n",
    "                        ai_responses += 1\n",
    "                \n",
    "                last_step_time = current_time\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Benchmark error: {e}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'iteration': iteration,\n",
    "            'total_time': total_time,\n",
    "            'total_steps': total_steps,\n",
    "            'tool_calls': tool_calls,\n",
    "            'ai_responses': ai_responses,\n",
    "            'avg_step_time': sum(step_times) / len(step_times) if step_times else 0,\n",
    "            'step_times': step_times\n",
    "        }\n",
    "    \n",
    "    def _calculate_averages(self, benchmarks: list):\n",
    "        \"\"\"Calculate average metrics across benchmark iterations.\"\"\"\n",
    "        \n",
    "        if not benchmarks:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'avg_total_time': sum(b['total_time'] for b in benchmarks) / len(benchmarks),\n",
    "            'avg_total_steps': sum(b['total_steps'] for b in benchmarks) / len(benchmarks),\n",
    "            'avg_tool_calls': sum(b['tool_calls'] for b in benchmarks) / len(benchmarks),\n",
    "            'avg_ai_responses': sum(b['ai_responses'] for b in benchmarks) / len(benchmarks),\n",
    "            'avg_step_time': sum(b['avg_step_time'] for b in benchmarks) / len(benchmarks),\n",
    "            'iterations': len(benchmarks)\n",
    "        }\n",
    "    \n",
    "    def _generate_performance_report(self):\n",
    "        \"\"\"Generate comprehensive performance report.\"\"\"\n",
    "        \n",
    "        if not self.benchmarks:\n",
    "            return {\"error\": \"No benchmark data available\"}\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_time = sum(b['avg_total_time'] for b in self.benchmarks)\n",
    "        total_steps = sum(b['avg_total_steps'] for b in self.benchmarks)\n",
    "        total_tool_calls = sum(b['avg_tool_calls'] for b in self.benchmarks)\n",
    "        \n",
    "        # Performance insights\n",
    "        insights = []\n",
    "        \n",
    "        avg_time_per_query = total_time / len(self.benchmarks)\n",
    "        if avg_time_per_query > 10:\n",
    "            insights.append(self.optimization_tips[\"high_latency\"])\n",
    "        \n",
    "        avg_tools_per_query = total_tool_calls / len(self.benchmarks)\n",
    "        if avg_tools_per_query > 3:\n",
    "            insights.append(self.optimization_tips[\"many_tool_calls\"])\n",
    "        \n",
    "        return {\n",
    "            \"summary\": {\n",
    "                \"total_queries\": len(self.benchmarks),\n",
    "                \"avg_time_per_query\": avg_time_per_query,\n",
    "                \"avg_steps_per_query\": total_steps / len(self.benchmarks),\n",
    "                \"avg_tools_per_query\": avg_tools_per_query,\n",
    "                \"total_benchmark_time\": total_time\n",
    "            },\n",
    "            \"optimization_insights\": insights,\n",
    "            \"detailed_results\": self.benchmarks\n",
    "        }\n",
    "\n",
    "# Run performance analysis\n",
    "analyzer = StreamingPerformanceAnalyzer()\n",
    "\n",
    "benchmark_queries = [\n",
    "    \"Calculate 5 + 3\",\n",
    "    \"Analyze text: 'Performance matters'\",\n",
    "    \"Search for AI news and calculate 10 * 2\"\n",
    "]\n",
    "\n",
    "print(\"Running performance benchmark...\")\n",
    "report = analyzer.benchmark_streaming(streaming_agent, benchmark_queries, iterations=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä PERFORMANCE REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"error\" not in report:\n",
    "    summary = report[\"summary\"]\n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    print(f\"  Total queries tested: {summary['total_queries']}\")\n",
    "    print(f\"  Average time per query: {summary['avg_time_per_query']:.2f}s\")\n",
    "    print(f\"  Average steps per query: {summary['avg_steps_per_query']:.1f}\")\n",
    "    print(f\"  Average tools per query: {summary['avg_tools_per_query']:.1f}\")\n",
    "    print(f\"  Total benchmark time: {summary['total_benchmark_time']:.2f}s\")\n",
    "    \n",
    "    if report[\"optimization_insights\"]:\n",
    "        print(f\"\\nüí° Optimization Insights:\")\n",
    "        for i, insight in enumerate(report[\"optimization_insights\"], 1):\n",
    "            print(f\"  {i}. {insight}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Performance looks good! No specific optimizations needed.\")\n",
    "else:\n",
    "    print(f\"‚ùå Benchmark failed: {report['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### When to Use Streaming\n",
    "\n",
    "1. **Long-Running Tasks**\n",
    "   - Complex research queries\n",
    "   - Multi-step workflows\n",
    "   - Document analysis\n",
    "   - Data processing tasks\n",
    "\n",
    "2. **Interactive Applications**\n",
    "   - Chat interfaces\n",
    "   - Real-time assistance\n",
    "   - Educational tutoring\n",
    "   - Code generation\n",
    "\n",
    "3. **User Experience Priorities**\n",
    "   - Immediate feedback desired\n",
    "   - Progress visibility important\n",
    "   - Early termination needed\n",
    "   - Transparency in AI reasoning\n",
    "\n",
    "### Stream Mode Selection\n",
    "\n",
    "#### \"values\" Mode\n",
    "- **Use when**: You need complete state at each step\n",
    "- **Benefits**: Full context, comprehensive debugging\n",
    "- **Drawbacks**: Higher bandwidth, more processing\n",
    "\n",
    "#### \"updates\" Mode  \n",
    "- **Use when**: You only need changes/deltas\n",
    "- **Benefits**: Lower bandwidth, efficient processing\n",
    "- **Drawbacks**: More complex state management\n",
    "\n",
    "#### \"messages\" Mode\n",
    "- **Use when**: You only care about message updates\n",
    "- **Benefits**: Minimal overhead, simple processing\n",
    "- **Drawbacks**: Limited context visibility\n",
    "\n",
    "### Implementation Guidelines\n",
    "\n",
    "1. **Error Handling**\n",
    "   ```python\n",
    "   try:\n",
    "       for chunk in agent.stream(query):\n",
    "           process_chunk(chunk)\n",
    "   except Exception as e:\n",
    "       handle_streaming_error(e)\n",
    "   ```\n",
    "\n",
    "2. **Performance Monitoring**\n",
    "   - Track step timing\n",
    "   - Monitor memory usage\n",
    "   - Measure tool call frequency\n",
    "   - Analyze user engagement\n",
    "\n",
    "3. **User Controls**\n",
    "   - Pause/resume functionality\n",
    "   - Early termination options\n",
    "   - Step-by-step mode\n",
    "   - Progress indicators\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "1. **Reduce Latency**\n",
    "   - Use faster models for simple tasks\n",
    "   - Optimize tool implementations\n",
    "   - Cache frequent operations\n",
    "   - Implement async processing\n",
    "\n",
    "2. **Manage Resources**\n",
    "   - Limit concurrent streams\n",
    "   - Implement timeouts\n",
    "   - Clean up completed streams\n",
    "   - Monitor memory usage\n",
    "\n",
    "3. **Improve User Experience**\n",
    "   - Provide meaningful progress updates\n",
    "   - Show estimated completion times\n",
    "   - Display intermediate results\n",
    "   - Allow user interruption\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Over-streaming**: Not every task benefits from streaming\n",
    "2. **Poor Error Handling**: Streams can fail at any point\n",
    "3. **Resource Leaks**: Clean up streams properly\n",
    "4. **UI Overwhelm**: Too much information can confuse users\n",
    "5. **Performance Issues**: Monitor and optimize continuously\n",
    "\n",
    "## Use Cases for Streaming\n",
    "\n",
    "### Research and Analysis\n",
    "- **Literature Reviews**: Stream progress through multiple papers\n",
    "- **Market Analysis**: Show real-time data gathering and processing\n",
    "- **Code Review**: Stream through different aspects of code analysis\n",
    "\n",
    "### Content Creation\n",
    "- **Document Writing**: Show outline creation, research, writing process\n",
    "- **Code Generation**: Display algorithm design, implementation, testing\n",
    "- **Report Building**: Stream data collection, analysis, visualization\n",
    "\n",
    "### Interactive Learning\n",
    "- **Step-by-step Tutorials**: Show problem-solving process\n",
    "- **Debugging Sessions**: Stream through troubleshooting steps\n",
    "- **Concept Explanation**: Build understanding incrementally\n",
    "\n",
    "### Business Applications\n",
    "- **Data Processing**: Show progress through large datasets\n",
    "- **Decision Support**: Stream through analysis factors\n",
    "- **Process Automation**: Display workflow execution steps\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Streaming responses transform agents from black boxes into transparent, interactive systems that:\n",
    "\n",
    "- **Engage Users**: Real-time feedback keeps users involved\n",
    "- **Build Trust**: Transparency in reasoning builds confidence\n",
    "- **Enable Control**: Users can guide and interrupt processes\n",
    "- **Improve Experience**: Immediate feedback feels more responsive\n",
    "- **Support Learning**: Seeing the process helps users understand\n",
    "\n",
    "Streaming is particularly powerful for complex, multi-step tasks where the journey is as important as the destination. It turns AI assistance from a request-response interaction into a collaborative problem-solving experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}